---
layout: post
title: 隐马尔可夫模型(HMM)
published: true
categories: MachineLearning
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

##  1. 隐马尔可夫模型(HMM)
隐马尔可夫模型是一个***序列模型(sequence model)***或者***序列分类器(sequence classifier)***。简单的说，序列模型就是给一个序列里的的每一个单元赋予一个标签或类别。举个栗子，在nlp任务中最常见的pos任务就是对一个由单词组成的语句序列标注词性。例：“HMM is a sequence model.” 对应的序列是“NNP,   VBZ, DT, NN, NN”。像这种序列标注任务在nlp中非常常见，除了POS, 还有NER 和 语音识别等。首先了解下马尔可夫链。
### 1.1 马尔可夫链（Markov Chain）
马尔可夫链可以看作是一个概率图模型，它表示了从一个状态转移到另一个状态的概率。它也可以看作一个一个加权有限自动机模型。因此，马尔可夫模型可以由一个三元组表示：

>1. **状态集合：\\( Q = q_1,q_2, ..., q_n \\)**    
>2. **状态概率转移矩阵：$$A = \begin{pmatrix}a_{01} & a_{02} & ... & a_{0n}\\\ \vdots & \vdots & \ddots & \vdots \\\ a_{n1} & a_{n2} & ... & a_{nn}\end{pmatrix}$$**    
>3. **开始状态和结束状态： \\( q_0, q_F \\)**          


我们在说马尔可夫链是通常是指一阶马尔可夫链，即：当前状态决定了下一个状态，与之前的状态无关。因此马尔可夫链有一个马尔可夫假设：
\\[ P(q_i | q_{i-1}, \dots, q_1) = P(q_i | q_{i-1})\\]
其中，从状态i转移到状态j的概率:\\(P(q_j|q_i) = a_{i,j} \\)

下图是单词的概率转移图示例：
<div style="text-align:center" markdown="1">
![马尔可夫链例子]({{site.baseurl}}/assets/a.png)
</div>

有了马尔可夫转移概率矩阵，很容易计算出一个序列的概率：比如序列 “\<start\> snow is white \<end\>” 的概率为：

\\[
 \begin{eqnarray}
  P(\<start\>,snow,is,white, \<end\>) & = & P(snow|start_0) P(is|snow  P(white|is)  P(white|end) \\
  & = & a_{02} a_{21}  a_{13}  a_{34}
 \end{eqnarray}
\\]

  
更多关于马尔可夫链参见：[https://en.wikipedia.org/wiki/Markov_chain](https://en.wikipedia.org/wiki/Markov_chain)

### 1.2 隐马尔可夫模型(HMM)：
有了马尔可夫链，很容易计算出一个序列的概率。例如序列：“snow is white”。这个能直接得到的序列我们称之为**观察变量（observed variable）**, 除了观察变量还有我们不能直接得到的变量比如，“snow is white”的part of speech tag序列，我们称之为**隐变量(hidden variable)**。隐马尔可夫模型既能计算已观察序列的概率又能计算隐变量序列的概率模型。


为了直观的理解HMM, 我们用类似《Speech and Language Processing》书中的一个示例来讲解HMM。假设，你想知道北京1942年6,7,8三个月的天气是热（HOT）还是冷(COLD)（假设只有冷和热两个状态），由于没有气象记录无法直接获得这些信息。但是你查到了那几个月份雪糕的销售量。因此，你的任务就是根据雪糕的销售量来推算出对应几个月分的天气情况。形式化的描述：你需要根据已观察变量序列（雪糕销量）来推断出隐变量序列（天气）。

首先看下HMM的形式化定义， HMM可以表示为一个五元组：

>1. **状态集合：\\( Q = q_1,q_2, ..., q_N \\)**    
>2. **状态概率转移矩阵：$$A = \begin{pmatrix}a_{01} & a_{02} & ... & a_{0n}\\\ \vdots & \vdots & \ddots & \vdots \\\ a_{n1} & a_{n2} & ... & a_{nn}\end{pmatrix}$$**   
>3. **已观察变量序列：\\( T = o_1,o_2, ..., o_T \\),** each one are drawn from Vocabulary \\(V=v_1, v_2, \cdots, v_n\\)  
>4. **观测概率矩阵（emission probabilities）：\\(B = b_i(0_t)\\)**,表示观测变量\\(o_t\\)由状态i产生的概率。   
>5. **开始状态和结束状态： \\( q_0, q_F \\)**          

对应到上文的例子：

>1. **状态集合：\\( Q = \{cold, hot\} \\)**  
>2. **状态概率转移矩阵：$$A = \begin{pmatrix}a_{00} & a_{01} & a_{02} & a_{03}\\\ a_{10} & a_{11} & a_{12} & a_{13} \\\ a_{20} & a_{21} & a_{22} & a_{23} \\\ a_{30} & a_{31} & a_{32} & a_{33} \end{pmatrix}$$**    
>3. **已观察变量序列：\\( T = 3, 1, 2 \\),** ， 表示6,7,8三个月份的雪糕销量分别是3,1,2。\\(V = (1,2,3) \\)   
>4. **观测概率矩阵（emission probabilities）：\\(B = b_i(0_t)\\)**,表示观测变量\\(o_t\\)由状态i产生的概率。 见图2    
>5. **开始状态和结束状态： \\( q_0, q_F \\)**          

![hmm]({{site.baseurl}}/assets/b.png)  
*图2：隐马尔可夫模型*

隐马尔可夫模型主要有三个问题：
>1. **likelyhood**: 基于HMM \\(\lambda=(A,B)\\)和观察变量序列 \\(O\\) 求出观察变量序列的似然概率:
\\(P(O|\lambda)\\)。
>2. **Decoding**:  基于HMM \\(\lambda=(A,B)\\)和观察变量序列 \\(O\\) 推断出最好的隐变量序列 \\(Q\\)。
>3. **Learning**:  基于观察变量序列和HMM的状态集语料，学习HMM的转移概率矩阵 \\(A\\) 和观测概率矩阵 \\(B\\)。


### 1.3 计算Likelyhood(求观察变量序列的概率)：Forward Algorithm
我们的目标就是计算已观察变量序列的概率，比如图2的HMM模型，计算序列 3,1,3的概率。如果是马尔可夫链，由于没有隐变量状态的存在，计算序列3,1,3的概率只需只需将3->1, 1->3的概率相乘即可。然而在隐马尔可夫模型中，由于每一个观察变量都来自一个隐变量，所以无法直接计算序列3,1,3的概率。但是加入我们知道了观察变量序列3,1,3 对应的隐变量序列是HOT, HOT, COLD，那么就容易根据HMM计算观察变量序列的概率, 如图3：
\\[P(3,1,3|hot,hot,cold) = P(3|hot)P(1|hot)P(3|cold)\\]
![cc]({{site.baseurl}}/assets/c.png)  
*图3：序列3,1,3基于隐变量序列hot,hot,cold的概率*  
由马尔可夫假设，推广到一般情况：
\\[P(O|Q) = \sum_{i}^{T}P(o_i|q_i)\\]  
但是一般情况下我们只有已观察变量序列，并没有隐变量序列，所以我们只能计算所有情况的隐变量序列，然后通过计算隐变量序列概率的加权和来得到观察序列的概率。有点绕，其实就是如下的一个计算公式：
\\[P(O) = \sum_Q P(O, Q) = \sum_{Q}P(O|Q)P(Q)\\]
\\[P(O,Q) = P(O|Q)P(Q) = \sum_{i}^{T}P(o_i|q_i)\times \sum_i^TP(q_i|q_{i-1})\\]  
所以上文中的3,1,3,hot,hot,cold的联合概率为：
\\[P(3,1,3,hot,hot,cold) = P(3|hot)P(1|hot)|p(3|cold)P(hot|start)P(hot|hot)P(cold|hot)\\]
由图2很容易计算出来。那么序列3,1,3的概率为：
\\[P(3,1,3) = P(3,1,3,cold, cold, cold) + P(3,1,3,cold,cold,hot) + \dots + \dots\\]

**但是但是但是**，假设HMM模型有N个隐变量状态，观察变量序列长度为T, 隐变量序列的可能情况有多少种呢？ \\(N^T\\)种。在实际应用中，N和T往往都很大，所以这个算法的时间复杂度太高了。Forward Algorithm 是一个动态规划算法，它通过存储中间临时值是该算法复杂度降低到了\\(N^2T\\)。话不多说直接看图4示例：

![ddd]({{site.baseurl}}/assets/d.png)
*图4：forward 算法示例*  
图4中**\\(\alpha_t(j) = P(o_1,o_2,\dots,o_t, q_t=j | \lambda)\\)**表示在看到前t个观察变量序列处于j状态的概率。那么  
![e.png]({{site.baseurl}}/assets/e.png)  
其中：  
\\(\alpha_{t-1}(i)\\):前向一个状态节点的概率。  
\\(a_{ij}\\):状态i到状态j的转换概率。  
\\(b_j(o_t)\\):观察变量\\(o_t\\)由状态j产生的概率。  

**Forward Algorithm：**  
简单讲，HMM forward算法有以下三个步骤：
![]({{site.baseurl}}/assets/f.png)  

![g.png]({{site.baseurl}}/assets/g.png)  
*图5：forward 算法*

### 1.4 Decoding: 维特比算法(Viterbi Algorithm)
HMM的decoding：给定一个观察变量序列，找到一个对应的最可能的隐变量状态序列。比如，在上文例子中，我们只知道北京6,7，8月份的雪糕销量是3,1,3， 那么根据改销量找到6,7，8月份的最可能的天气序列。

对，HMM 的 decoding算法是Viterbi算法，它和前向算法（Forward algorithm）一样也是一种动态规划算法。话不多说，以雪糕例子为例，直接上图：  
![ff.png]({{site.baseurl}}/assets/ff.png)
*图6：Viterbi算法示例，圆圈代表状态，虚线圆圈表示非法状态，方框表示观察变量序列。*  

在上图中，每一个圆圈的概率\\(v_t(j) = max_{q_0,\dots,q_t}P(q_0,\dots,q_{t-1}, o_0,\dots, o_t,q_t = j | \lambda)\\)，通过前面每一步的最可能的路径找到最优的隐变量状态序列，即每一个状态的概率由如下计算：
\\[ v_t(j) = max_{q_1, \dots, q_{t-1}}P(q_0, \dots, q_{t-1}, o_0, \dots, o_t, q_t = j | \lambda)
\\]  
像forward算法一样，我们可以递归的计算，对于t个观察变量，隐状态\\(q_j\\):
\\[v_t(j) = max_{i=1}^{N}v_{t-1}(i)a_{i,j}b_j(o_t_)\\]  



![vv.png]({{site.baseurl}}/assets/vv.png)
*图7： Viterbi算法*
### 1.5 HMM 训练： The Forward-Backward Algorithm
